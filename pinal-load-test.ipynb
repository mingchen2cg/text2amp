{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f19a50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from models.pinal_module.T2stru_loading import load_T2Struc\n",
    "\n",
    "# DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \n",
    "\n",
    "# T2Struc, text_tokenizer, structure_tokenizer = load_T2Struc(\"/t9k/mnt/CM/text2amp/weights/T2struc-fined/2025-11-27_21-54-10/final_model\", dtype=torch.float16, device=DEVICE)\n",
    "# #\n",
    "# print(T2Struc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad7cb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from models.progen3_module.modeling import ProGen3ForCausalLM\n",
    "\n",
    "# progen = ProGen3ForCausalLM.from_pretrained(\"./weights/progen3-762m\", torch_dtype=torch.float16, device_map=\"cuda\")\n",
    "\n",
    "# print(progen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7bee12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import shutil\n",
    "# import torch\n",
    "\n",
    "# from transformers import T5EncoderModel\n",
    "# from models.pinal_module.T2stru_loading import load_T2Struc\n",
    "\n",
    "\n",
    "# def export_lm_only(\n",
    "#     t2struc_final_model_dir: str,\n",
    "#     pinal_official_t5_large_ckpt_dir: str,\n",
    "#     lm_out_dir: str,\n",
    "#     device: torch.device,\n",
    "#     dtype: torch.dtype = torch.float16,\n",
    "# ):\n",
    "#     \"\"\"\n",
    "#     导出 T2Struc.lm (T5EncoderModel) 为独立 HuggingFace 格式目录：\n",
    "#       lm_out_dir/\n",
    "#         - config.json\n",
    "#         - model.safetensors 或 pytorch_model.bin\n",
    "#         - tokenizer相关文件（tokenizer.json / spiece.model / special_tokens_map.json / tokenizer_config.json 等）\n",
    "\n",
    "#     注意：\n",
    "#     - pinal_official_t5_large_ckpt_dir：用于在 tokenizer 无法 save_pretrained 时回退复制相关文件，\n",
    "#       其语义是“PINAL authors 官方训练并发布的 T5-large 子模块/权重目录”。\n",
    "#     \"\"\"\n",
    "#     os.makedirs(lm_out_dir, exist_ok=True)\n",
    "\n",
    "#     # 1) 按现有方式加载完整模型（确保 lm 权重已被正确加载）\n",
    "#     T2Struc, text_tokenizer, structure_tokenizer = load_T2Struc(\n",
    "#         t2struc_final_model_dir,\n",
    "#         dtype=dtype,\n",
    "#         device=device,\n",
    "#     )\n",
    "#     lm: T5EncoderModel = T2Struc.lm\n",
    "#     lm.eval()\n",
    "\n",
    "#     # 2) 保存 lm 权重 + config（HF 标准）\n",
    "#     #    safe_serialization=True 会优先产出 model.safetensors（环境没装 safetensors 时会退回 pytorch_model.bin）\n",
    "#     lm.save_pretrained(lm_out_dir, safe_serialization=True)\n",
    "\n",
    "#     # 3) 保存 tokenizer（优先用 text_tokenizer.save_pretrained；不行就复制 pinal_official_t5_large_ckpt_dir 里的 tokenizer 文件）\n",
    "#     saved_tok = False\n",
    "#     if hasattr(text_tokenizer, \"save_pretrained\"):\n",
    "#         try:\n",
    "#             text_tokenizer.save_pretrained(lm_out_dir)\n",
    "#             saved_tok = True\n",
    "#         except Exception as e:\n",
    "#             print(\n",
    "#                 \"[WARN] text_tokenizer.save_pretrained 失败，将回退复制 pinal_official_t5_large_ckpt_dir。\"\n",
    "#                 f\"原因: {e}\"\n",
    "#             )\n",
    "\n",
    "#     if not saved_tok:\n",
    "#         # 把 pinal-official-t5-large 目录里的 tokenizer & config 相关文件复制过来\n",
    "#         # 注意：lm.save_pretrained 已经写了 config.json；这里复制时允许覆盖/合并\n",
    "#         for fn in [\n",
    "#             \"config.json\",\n",
    "#             \"special_tokens_map.json\",\n",
    "#             \"spiece.model\",\n",
    "#             \"tokenizer_config.json\",\n",
    "#             \"tokenizer.json\",\n",
    "#         ]:\n",
    "#             src = os.path.join(pinal_official_t5_large_ckpt_dir, fn)\n",
    "#             if os.path.exists(src):\n",
    "#                 shutil.copy2(src, os.path.join(lm_out_dir, fn))\n",
    "\n",
    "#     print(f\"[OK] lm 已导出到: {lm_out_dir}\")\n",
    "#     print(f\"[OK] 导出目录文件列表: {sorted(os.listdir(lm_out_dir))}\")\n",
    "\n",
    "\n",
    "# def main():\n",
    "#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#     # 原始 T2Struc 最终模型目录（包含完整权重）\n",
    "#     t2struc_final_model_dir = \"/t9k/mnt/CM/weights/T2struc-1.2B\"\n",
    "\n",
    "#     # PINAL authors 官方训练并发布的 T5-large 子模块/权重目录（用于 tokenizer 回退复制）\n",
    "#     pinal_official_t5_large_ckpt_dir = \"/t9k/mnt/CM/text2amp/weights/pinal-official-t5-large/\"\n",
    "\n",
    "#     # 导出目录（建议不要和源目录同一路径，避免覆盖/混淆）\n",
    "#     lm_out_dir = \"/t9k/mnt/CM/text2amp/weights/pinal-official-t5-large/\"\n",
    "\n",
    "#     export_lm_only(\n",
    "#         t2struc_final_model_dir=t2struc_final_model_dir,\n",
    "#         pinal_official_t5_large_ckpt_dir=pinal_official_t5_large_ckpt_dir,\n",
    "#         lm_out_dir=lm_out_dir,\n",
    "#         device=device,\n",
    "#         dtype=torch.float16,\n",
    "#     )\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69c1f698",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/t9k/mnt/.conda/envs/ref-progen3/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from models.T5Encoder.t5_encoder_pinal import T5Encoder\n",
    "\n",
    "def load_lm_only(\n",
    "    lm_dir: str,\n",
    "    device: torch.device,\n",
    "    dtype: torch.dtype = torch.float16,\n",
    "):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(lm_dir, use_fast=True)\n",
    "    model = T5Encoder(lm_dir=lm_dir, device=device, dtype=dtype)\n",
    "    return model, tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e143db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 1024])\n",
      "T5Encoder(\n",
      "  (lm): T5EncoderModel(\n",
      "    (shared): Embedding(32128, 1024)\n",
      "    (encoder): T5Stack(\n",
      "      (embed_tokens): Embedding(32128, 1024)\n",
      "      (block): ModuleList(\n",
      "        (0): T5Block(\n",
      "          (layer): ModuleList(\n",
      "            (0): T5LayerSelfAttention(\n",
      "              (SelfAttention): T5Attention(\n",
      "                (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "                (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "                (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "                (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "                (relative_attention_bias): Embedding(32, 16)\n",
      "              )\n",
      "              (layer_norm): T5LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (1): T5LayerFF(\n",
      "              (DenseReluDense): T5DenseActDense(\n",
      "                (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "                (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (act): ReLU()\n",
      "              )\n",
      "              (layer_norm): T5LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1-23): 23 x T5Block(\n",
      "          (layer): ModuleList(\n",
      "            (0): T5LayerSelfAttention(\n",
      "              (SelfAttention): T5Attention(\n",
      "                (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "                (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "                (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "                (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "              )\n",
      "              (layer_norm): T5LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (1): T5LayerFF(\n",
      "              (DenseReluDense): T5DenseActDense(\n",
      "                (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "                (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (act): ReLU()\n",
      "              )\n",
      "              (layer_norm): T5LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (final_layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "lm_dir = \"/t9k/mnt/CM/text2amp/weights/pinal-official-t5-large\"\n",
    "\n",
    "model, tokenizer = load_lm_only(lm_dir, DEVICE, dtype=torch.float16)\n",
    "\n",
    "batch = tokenizer(\n",
    "    [\"hello world\", \"this is a test\"],\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "batch = {k: v.to(DEVICE) for k, v in batch.items()}\n",
    "\n",
    "# 1) 最常用：只拿最后一层 hidden states\n",
    "with torch.no_grad():\n",
    "    last_hidden = model(\n",
    "        input_ids=batch[\"input_ids\"],\n",
    "        attention_mask=batch[\"attention_mask\"],\n",
    "        return_dict=False,   # 直接返回 Tensor\n",
    "    )\n",
    "\n",
    "print(last_hidden.shape)  # (B, L, H) -> (2, seq_len, 1024)\n",
    "\n",
    "print(model)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ref-progen3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
