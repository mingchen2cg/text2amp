{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd97c473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import os\n",
    "# from models.Text2ProGenModel import Text2AmpModel\n",
    "\n",
    "# # 1. 基础配置\n",
    "# # 注意：这里的路径需要替换为你实际环境中的路径\n",
    "# config = {\n",
    "#     \"lm\": \"/t9k/mnt/CM/text2amp/weights/pinal-official-t5-large\", # T5 模型路径\n",
    "#     \"plm_type\": \"./weights/progen3-762m\",                          # ProGen3 模型路径\n",
    "# }\n",
    "\n",
    "# # 检查路径\n",
    "# if not os.path.exists(config[\"lm\"]) or not os.path.exists(config[\"plm_type\"]):\n",
    "#     print(\"Warning: Model paths not found. Please set correct paths in 'config'.\")\n",
    "#     # 如果路径不存在，接下来的代码可能会报错，建议在此处检查环境\n",
    "# else:\n",
    "#     # 2. 实例化模型\n",
    "#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#     model = Text2AmpModel(config)\n",
    "#     model.to(device)\n",
    "#     model.train() # 设置为训练模式\n",
    "# #\n",
    "#     print(\"\\n[Status] Model Architecture constructed successfully.\")\n",
    "#     print(f\"[Info] Total layers in ProGen3: {len(model.plm.model.layers)}\")\n",
    "    \n",
    "#     # 3. 构造虚拟数据 (Dummy Data)\n",
    "#     # B: Batch size, L_text: 文本长度, L_prot: 蛋白质序列长度\n",
    "#     B, L_text, L_prot = 2, 10, 20\n",
    "    \n",
    "#     # 模拟 T5 Tokenizer 的输出 (文本 ID 和 Mask)\n",
    "#     text_ids = torch.randint(0, 1000, (B, L_text)).to(device)\n",
    "#     text_masks = torch.ones((B, L_text)).to(device)\n",
    "    \n",
    "#     # 模拟 ProGen3 Tokenizer 的输出 (蛋白质结构 Token ID)\n",
    "#     structure_token_ids = torch.randint(0, 100, (B, L_prot)).to(device)\n",
    "    \n",
    "#     batch = {\n",
    "#         \"text_ids\": text_ids,\n",
    "#         \"text_masks\": text_masks,\n",
    "#         \"structure_token_ids\": structure_token_ids,\n",
    "#         \"labels\": structure_token_ids  # 常用作计算 CrossEntropy Loss 的标签\n",
    "#     }\n",
    "    \n",
    "#     # 4. 执行前向传播 (Forward Pass)\n",
    "#     print(\"\\n[Action] Running forward pass...\")\n",
    "#     outputs = model(batch)\n",
    "    \n",
    "#     # 5. 输出结果\n",
    "#     print(\"-\" * 30)\n",
    "#     print(f\"Loss: {outputs['loss'].item():.4f}\")\n",
    "#     print(f\"Logits shape: {outputs['logits'].shape}\")\n",
    "#     print(\"-\" * 30)\n",
    "    \n",
    "#     # 6. 验证 Cross-Attention 梯度状态\n",
    "#     print(\"\\n[Check] Verifying gradient status:\")\n",
    "#     for name, param in model.named_parameters():\n",
    "#         if \"cross_attn\" in name:\n",
    "#             print(f\"-> {name}: requires_grad={param.requires_grad}\")\n",
    "#             break # 只打印第一个 cross_attn 示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48b488f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 在模型初始化后，开始训练循环前运行这段代码\n",
    "\n",
    "# # 1. 冻结 T5 Encoder (通常只作为特征提取器)\n",
    "# for param in model.lm.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "# # 2. 冻结 ProGen3 的原始参数\n",
    "# # 技巧：我们只让名字里带有 \"cross_attn\" 的参数保持 requires_grad=True\n",
    "# for name, param in model.plm.named_parameters():\n",
    "#     if \"cross_attn\" in name:\n",
    "#         param.requires_grad = True\n",
    "#         print(f\"Training: {name}\") # 打印一下确认哪些层在训练\n",
    "#     else:\n",
    "#         param.requires_grad = False\n",
    "\n",
    "# # 检查一下可训练参数量\n",
    "# trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "# all_params = sum(p.numel() for p in model.parameters())\n",
    "# print(f\"\\nTrainable params: {trainable_params} / {all_params} ({trainable_params/all_params:.2%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b8a696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b106aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "\n",
    "# # 定义保存路径\n",
    "# save_path = \"./weights/text2protein_model/model.pt\"\n",
    "# os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "\n",
    "# print(\"正在保存完整模型检查点...\")\n",
    "\n",
    "# # 我们构造一个包含所有必要信息的字典\n",
    "# # 这样新的类就不需要再依赖原始的 T5/ProGen3 权重路径了\n",
    "# checkpoint = {\n",
    "#     # 1. 核心权重\n",
    "#     \"state_dict\": model.state_dict(),\n",
    "    \n",
    "#     # 2. 两个子模型的配置 (用于构建空骨架)\n",
    "#     # 注意：我们提取底层的 config 对象\n",
    "#     \"t5_config\": model.lm.lm.config, \n",
    "#     \"progen_config\": model.plm.config,\n",
    "    \n",
    "#     # 3. 其他辅助信息\n",
    "#     \"lm_dim\": model.lm_dim,\n",
    "#     \"model_config\": model.config\n",
    "# }\n",
    "\n",
    "# torch.save(checkpoint, save_path)\n",
    "# print(f\"模型已成功保存至: {save_path}\")\n",
    "# print(\"后续请使用新生成的 Text2ProteinGenModel 类加载此文件。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edae05fd",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67709c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from models.Text2ProteinGenModel import Text2ProteinGenModel\n",
    "\n",
    "# # 直接加载，不需要传任何 config 字典\n",
    "# model = Text2ProteinGenModel(\"./weights/text2protein_model/model.pt\")\n",
    "# model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438936a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from transformers import PreTrainedModel\n",
    "\n",
    "# # 定义保存路径\n",
    "# save_path = \"text2protein_complete.pt\"\n",
    "\n",
    "# print(f\"正在打包保存完整模型到 {save_path} ...\")\n",
    "\n",
    "# # 1. 获取 T5 Config 的辅助函数 (自动判断是 model.lm.config 还是 model.lm.lm.config)\n",
    "# def get_t5_config(model):\n",
    "#     if hasattr(model.lm, \"config\"):\n",
    "#         return model.lm.config\n",
    "#     elif hasattr(model.lm, \"lm\") and hasattr(model.lm.lm, \"config\"):\n",
    "#         return model.lm.lm.config\n",
    "#     else:\n",
    "#         raise ValueError(\"无法找到 T5 模型的 config，请检查模型结构\")\n",
    "\n",
    "# # 2. 获取 ProGen3 Config 的辅助函数\n",
    "# def get_progen_config(model):\n",
    "#     if hasattr(model.plm, \"config\"):\n",
    "#         return model.plm.config\n",
    "#     elif hasattr(model.plm, \"model\") and hasattr(model.plm.model, \"config\"):\n",
    "#         return model.plm.model.config\n",
    "#     else:\n",
    "#         # 尝试直接从 model.config 获取（如果你之前的代码有赋值）\n",
    "#         return model.config\n",
    "\n",
    "# t5_config = get_t5_config(model)\n",
    "# progen_config = get_progen_config(model)\n",
    "\n",
    "# checkpoint = {\n",
    "#     # 1. 完整的状态字典\n",
    "#     \"state_dict\": model.state_dict(),\n",
    "    \n",
    "#     # 2. 结构配置\n",
    "#     \"config\": {\n",
    "#         \"t5_config\": t5_config,\n",
    "#         \"progen_config\": progen_config,\n",
    "#         \"lm_dim\": model.lm_dim,\n",
    "#         \"vocab_size\": progen_config.vocab_size \n",
    "#     }\n",
    "# }\n",
    "\n",
    "# torch.save(checkpoint, save_path)\n",
    "# print(\"保存成功！\")\n",
    "# print(f\"T5 Config hidden size: {t5_config.d_model}\")\n",
    "# print(f\"ProGen3 Config hidden size: {progen_config.hidden_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c3c819b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/t9k/mnt/.conda/envs/ref-progen3/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/t9k/mnt/.conda/envs/ref-progen3/lib/python3.12/site-packages/flash_attn/ops/triton/layer_norm.py:984: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd\n",
      "/t9k/mnt/.conda/envs/ref-progen3/lib/python3.12/site-packages/flash_attn/ops/triton/layer_norm.py:1043: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @custom_bwd\n",
      "/t9k/mnt/CM/text2amp/models/Text2ProteinGenModel.py:131: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location=\"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在从 text2protein_complete.pt 加载模型...\n",
      "Loading model configuration from text2protein_complete.pt...\n",
      "Building model architecture...\n",
      "Loading state dictionary...\n",
      "Model restored successfully!\n",
      "\n",
      "=== 模型加载成功！检查信息如下 ===\n",
      "运行设备: cuda\n",
      "T5 文本维度: 1024\n",
      "ProGen3 词表大小: 134\n",
      "\n",
      "第一层结构类型: <class 'models.Text2ProteinGenModel.ProGen3LayerWithCrossAttn'>\n",
      "✅ Cross-Attention 模块已检测到 (注入成功)\n",
      "\n",
      "正在运行测试推理 (Dummy Forward)...\n",
      "✅ 推理成功！Loss: 16.0000\n",
      "Logits Shape: torch.Size([1, 20, 134])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from models.Text2ProteinGenModel import Text2ProteinGenModel\n",
    "\n",
    "# 1. 指定权重文件路径\n",
    "checkpoint_path = \"text2protein_complete.pt\"\n",
    "\n",
    "print(f\"正在从 {checkpoint_path} 加载模型...\")\n",
    "\n",
    "# 2. 实例化并加载 (不需要任何额外的 config 配置，全在 pt 文件里)\n",
    "model = Text2ProteinGenModel(checkpoint_path)\n",
    "\n",
    "# 3. 转移到 GPU 并设为评估模式\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(\"\\n=== 模型加载成功！检查信息如下 ===\")\n",
    "print(f\"运行设备: {device}\")\n",
    "print(f\"T5 文本维度: {model.lm_dim}\")\n",
    "print(f\"ProGen3 词表大小: {model.plm.config.vocab_size}\")\n",
    "\n",
    "# 4. 检查一下层结构，确认 CrossAttention 存在\n",
    "first_layer = model.plm.model.layers[0]\n",
    "print(f\"\\n第一层结构类型: {type(first_layer)}\")\n",
    "if hasattr(first_layer, 'cross_attn'):\n",
    "    print(\"✅ Cross-Attention 模块已检测到 (注入成功)\")\n",
    "else:\n",
    "    print(\"❌ 未检测到 Cross-Attention (结构错误)\")\n",
    "\n",
    "# 5. 简单跑一个 Dummy Forward 验证一切正常\n",
    "print(\"\\n正在运行测试推理 (Dummy Forward)...\")\n",
    "try:\n",
    "    dummy_text = torch.randint(0, 100, (1, 10)).to(device)\n",
    "    dummy_prot = torch.randint(0, 100, (1, 20)).to(device)\n",
    "    \n",
    "    batch = {\n",
    "        \"text_ids\": dummy_text,\n",
    "        \"text_masks\": torch.ones_like(dummy_text),\n",
    "        \"protein_ids\": dummy_prot, # 注意这里用了修正后的键名\n",
    "        \"labels\": dummy_prot\n",
    "    }\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(batch)\n",
    "    \n",
    "    print(f\"✅ 推理成功！Loss: {output['loss'].item():.4f}\")\n",
    "    print(f\"Logits Shape: {output['logits'].shape}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ 推理出错: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ref-progen3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
